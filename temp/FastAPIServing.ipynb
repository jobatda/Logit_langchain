{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개요 \n",
    "1. FastAPI 특징 및 주요 기능 간단 설명\n",
    "2. llm 서버 주의사항?\n",
    "3. langserve 있다는거 알려줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. fastAPI에 llm 호출 서빙\n",
    "\n",
    "### 테스트 편의를 위하여 `get`만 사용하며, 필요 인자는 쿼리 매개변수로 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from fastapi.responses import StreamingResponse\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# FastAPI 인스턴스 생성\n",
    "\n",
    "app = FastAPI()\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "chain = llm | StrOutputParser()\n",
    "\n",
    "\n",
    "@app.get(\"/invoke\")\n",
    "def sync_chat(message: str):\n",
    "    response = chain.invoke(message)\n",
    "    return response\n",
    "\n",
    "\n",
    "@app.get(\"/ainvoke\")\n",
    "async def async_chat(message: str):\n",
    "    response = await chain.ainvoke(message)\n",
    "    return response\n",
    "\n",
    "\n",
    "@app.get(\"/stream\")\n",
    "def sync_stream_chat(message: str):\n",
    "    def event_stream():\n",
    "        try:\n",
    "            for chunk in chain.stream(message):\n",
    "                if len(chunk) > 0:\n",
    "                    yield f\"{chunk}\"\n",
    "        except Exception as e:\n",
    "            yield f\"data: {str(e)}\\n\\n\"\n",
    "\n",
    "    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\n",
    "\n",
    "\n",
    "@app.get(\"/astream\")\n",
    "async def async_stream_chat(message: str):\n",
    "    async def event_stream():\n",
    "        try:\n",
    "            async for chunk in chain.astream(message):\n",
    "                if len(chunk) > 0:\n",
    "                    yield f\"{chunk}\"\n",
    "        except Exception as e:\n",
    "            yield f\"data: {str(e)}\\n\\n\"\n",
    "\n",
    "    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "밑의 코드를 통해서 FastAPI를 실행시키고 당신의 브라우저에서 url로 접속하여 테스트 해보시오\n",
    "\n",
    "ex: `http://127.0.0.1:8000/invoke?message=구글의 설립연도를 알려줘`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. FastAPI VectorDB 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "DB_PATH = \"./chroma_db\"\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "chroma = Chroma(\n",
    "    collection_name=\"FastApiServing\",\n",
    "    persist_directory=DB_PATH,\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "retriever = chroma.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 4,\n",
    "    }\n",
    ")\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "@app.get(\"/invoke\")\n",
    "def sync_chat(message: str):\n",
    "    response = chain.invoke(message)\n",
    "    return response\n",
    "\n",
    "\n",
    "@app.get(\"/add-content\")\n",
    "async def add_content(content: str, author: str):\n",
    "    time.sleep(3)\n",
    "    chroma.add_texts([content], [{\"source\": author}])\n",
    "    return {\"message\": f\"add-content: {content}\"}\n",
    "\n",
    "\n",
    "@app.get(\"/async-add-content\")\n",
    "async def async_add_content(content: str, author: str):\n",
    "    await asyncio.sleep(3)\n",
    "    await chroma.aadd_texts([content], [{\"source\": author}])\n",
    "    return {\"message\": f\"async-add-content: {content}\"}\n",
    "\n",
    "\n",
    "# 추가 예정\n",
    "# get by ids()\n",
    "# aget by ids()\n",
    "\n",
    "# delete\n",
    "# adelete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'text': 'dict' object cannot be converted to 'PyString'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m봉암 수원지\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdepts1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m경남\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdepts2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m진주시\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdepts3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m봉암면\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimgUrl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./imgs/boongamImg.jpeg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/logitProject/Logit_Infra/llmserver/Logit_LANGCHAIN/.venv/lib/python3.13/site-packages/langchain_chroma/vectorstores.py:532\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 532\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[1;32m    536\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[0;32m~/Documents/logitProject/Logit_Infra/llmserver/Logit_LANGCHAIN/.venv/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:588\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    587\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/logitProject/Logit_Infra/llmserver/Logit_LANGCHAIN/.venv/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:480\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03mGenerate length-safe embeddings for a list of texts.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;124;03m    List[List[float]]: A list of embeddings for each input text.\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    479\u001b[0m _chunk_size \u001b[38;5;241m=\u001b[39m chunk_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size\n\u001b[0;32m--> 480\u001b[0m _iter, tokens, indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n",
      "File \u001b[0;32m~/Documents/logitProject/Logit_Infra/llmserver/Logit_LANGCHAIN/.venv/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:441\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._tokenize\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    439\u001b[0m     token \u001b[38;5;241m=\u001b[39m encoding\u001b[38;5;241m.\u001b[39mencode(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoder_kwargs)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[43mencoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_ordinary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Split tokens into chunks respecting the embedding_ctx_length\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(token), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ctx_length):\n",
      "File \u001b[0;32m~/Documents/logitProject/Logit_Infra/llmserver/Logit_LANGCHAIN/.venv/lib/python3.13/site-packages/tiktoken/core.py:69\u001b[0m, in \u001b[0;36mEncoding.encode_ordinary\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Encodes a string into tokens, ignoring special tokens.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03mThis is equivalent to `encode(text, disallowed_special=())` (but slightly faster).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m[31373, 995]\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core_bpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_ordinary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# See comment in encode\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'text': 'dict' object cannot be converted to 'PyString'"
     ]
    }
   ],
   "source": [
    "chroma.add_texts(\n",
    "    [\n",
    "        \"봉암 수원지\",\n",
    "    ],\n",
    "    \n",
    "    metadata=[{\n",
    "            \"depts1\": \"경남\",\n",
    "            \"depts2\": \"진주시\",\n",
    "            \"depts3\": \"봉암면\",\n",
    "            \"imgUrl\": \"./imgs/boongamImg.jpeg\",\n",
    "        },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = retriever.invoke(\"TF IDF 에 대하여 알려줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='2e79119d-f90f-49ca-a467-7a8d79e087e8', metadata={'source': '이동학'}, page_content='안녕하세용가리'), Document(id='39250963-d3fe-4b22-907f-f66f216b6e18', metadata={'source': '이동학'}, page_content='안녕하세용가리가리'), Document(id='33046dc2-49ef-4322-9203-4397f86b183f', metadata={'source': '이동학'}, page_content='안녕하세용가리가리'), Document(id='06fd1ee4-50cb-4324-adf2-41014403ef6c', metadata={'source': '이동학'}, page_content='안녕하세용가리가리')]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex: `http://127.0.0.1:8000/async-add-content?content=Hello World&author=donghak`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FastAPI 서빙시 비동기 중요성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from fastapi import FastAPI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "\n",
    "DB_PATH = \"./chroma_db\"\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "chroma = Chroma(\n",
    "    collection_name=\"FastApiServing\",\n",
    "    persist_directory=DB_PATH,\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "retriever = chroma.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 4,\n",
    "    }\n",
    ")\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "@app.get(\"/invoke\")\n",
    "def sync_chat(message: str):\n",
    "    response = chain.invoke(message)\n",
    "    return response\n",
    "\n",
    "\n",
    "@app.get(\"/add-content\")\n",
    "async def add_content(content: str, author: str):\n",
    "    time.sleep(3)\n",
    "    # chroma.add_texts([content], [{\"source\": author}])\n",
    "    return {\"message\": f\"add-content: {content}\"}\n",
    "\n",
    "\n",
    "@app.get(\"/async-add-content\")\n",
    "async def async_add_content(content: str, author: str):\n",
    "    await asyncio.sleep(3)\n",
    "    # await chroma.aadd_texts([content], [{\"source\": author}])\n",
    "    return {\"message\": f\"async-add-content: {content}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비동기 비교 테스트 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import httpx\n",
    "import time\n",
    "\n",
    "\n",
    "async def make_request(endpoint):\n",
    "    url = f\"http://127.0.0.1:8000/{endpoint}\"\n",
    "\n",
    "    # print(f\"-----url: {url}------\")\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=20) as client:\n",
    "        response = await client.get(url)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Response from {endpoint}: {response.json()}\")\n",
    "        else:\n",
    "            print(f\"Error from {endpoint}: {response.status_code}\")\n",
    "            print(\"Response content:\", response.text)\n",
    "\n",
    "\n",
    "async def main(endpoint):\n",
    "    tasks = [\n",
    "        make_request(f\"{endpoint}?content=my name is donghak&author=donghak\"),\n",
    "        make_request(\"invoke?message=내이름을 하나만 말해봐\"),\n",
    "    ]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Test sync\n",
    "    start_time = time.time()\n",
    "    for _ in range(3):\n",
    "        asyncio.run(main(endpoint=\"add-content\"))\n",
    "    end_time = time.time()\n",
    "    syncResult = end_time - start_time\n",
    "\n",
    "    #  test async\n",
    "    start_time = time.time()\n",
    "    for _ in range(3):\n",
    "        asyncio.run(main(endpoint=\"async-add-content\"))\n",
    "    end_time = time.time()\n",
    "    asyncResult = end_time - start_time\n",
    "\n",
    "    print(f\"Sync time: {syncResult}\")\n",
    "    print(f\"Async time: {asyncResult}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
